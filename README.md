# Task 2 Methods
Simulating mutations and testing variant callers.

## Part 1: Validating Data

Note: this scripts do not work on the command line, you need to load them and manually edit in filenames (and change the output file names if desired) in order to use them.

### Making Mutations:
mutations.py contains a function called mutations(), which takes in a .fasta file filename e.g. mutations('name.fasta') and outputs both a change log and a mutated genome in .txt format.

The function works broadly by first performing deletions, then insertions, and then single nucleotide polymorphisms (SNPs). Throughout the function, a 'mutation pool' is maintained which includes all available nucleotides for mutation (those which have not already been inserted, deleted, or SNPed). To do this, a 'reference index' is established, labelling all characters in the input sequence from 1 onwards. During deletion, the index points corresponding to the deleted nucleotides are deleted from the index, and during insertion, a list of 0s matching the length of the insertion are inserted into the index. Before each transformation, a point for the INDEL/SNP is selection from the mutation pool, and in this way, each INDEL/SNP is distinct, and non-overlapping. SNPs are selected all at once, using random.sample() from the random package, which selects distinct samples to prevent a SNP occuring at the same point. 

The function begins by prepping the FASTA file, removing the ID header and \n newline characters to read in the sequence as string 'seq'. Each mutation made to the genome is recorded in a dictionary object 'change_log'.

#### Deletions:
Deletions are performed in a loop, interating for the number of deletions assigned in the beginning. In each iteration, a random point is selected from the reference index, and the index point of the selected point of deletion relative to the reference index is taken using .index(). While this seems redundant on the first loop, this is how the changelog remains consistent relative to the original sequence despite frameshift mutations.

A random deletion length is generated from 1-10bp, which is added to the index point previously selected to select a small section of the sequence to be deleted, called 'del_seq'. An if/else statement is included to check wether the deletion will overhang the end of the sequence. If this is the case, then the deletion will only occur until the end of the sequence, rather than the full deletion length. Finally, the changelog, sequence, and reference index are all updated.

#### Insertions:
The reference index is first filtered to remove all 0 points (which relate to previous insertions) to prevent overlapping insertions. A random length of 1-10bp in generated by randomly selecting a nucleotide using random.choice() from the random package in a loop, adding a nucelotide each time until the desired length is reached.

As before, a random point is selected, and the relative index point if extracted. The changelog and reference index is updated (by inserting list of 0s the length of mutation, at the mutation point), and sequence is inserted at the point selected.

#### SNPs:
As iterating over the genome 300 times would be computationally expensive, all 300 SNP nucleotides are chosen simultaneously from the (filtered so as to prevent SNPs of insertion seqeunces) reference index. Rather than looping through all 300 points and selecting a SNP for each nucleotide, 300 random bases are generated at once. These bases are then compared in a while loop/if statement combination, where SNPs are again randomly chosen if they are the same as the original nucleotide. The idea behind this is that there are fewer changes to be made to the sequence, as each time, you would expect only ~25% of SNPs to be the same as the orignal nucelotide. By only updating SNPs that match, and preventing for-loop iteration across the entire genome or across 300 nucleotides, this should run more efficiently.
The SNPs are then applied to the genome, and the change log is updated.

Finally, the mutated sequence and changelog (ordered by coordinate) are output. 

### Simulating Reads:
reads.py contains function reads(), which takes in a .txt file in fasta format, a desired read length, and a desired average depth, and will return a fastq file in .fq format. 

The function takes in the sequence file, as well as a desired read length and read depth. It begins by reading in the sequence and spliting the header from the nucleotide codes. The number of reads to generated is calculated by dividing the rounded sequence length by the input read length and then multiplying by the depth, to approximate the number of reads required to reach the desired depth.

An index of the sequence is then generated, and a random selection of points (using random.choices() from the random package) are selected, dependent on the number or reads to be generated. The points are then extended to cover the full read lengths, and any points which result in reads hanging over the end of the genome are set to the latest possible read. The reads are then extracted from the main sequence using the index. 

The FASTA file header is edited to FASTQ format, and random quality scores are generated (these do not have meaning, but were included for the sake of realistic formating). The output file is written as standard FASTQ format with four lines: a unique ID per read, the read sequence, a '+' symbol, and the quality scores.

### Compressing the FASTQ File:

Although this step is not absolutely necessary, compressing the (very large) FASTQ file allows for easier sharing and storage, which is good practice for future projects. This project used the open-source software gzip within a command line conda envrionment to compress the file, using the command

  > gzip filename.fq

To output the file in filename.fq.gz format. This allowed for easier processing during the next stages of the project.

### Mapping the Reads:

minimap2 was used to map the simulated reads to the original reference genome. 

  > minimap2 -a reference.fasta reads.fq.gz | samtools view -h -F 0x900 - | samtools sort -0 bam > output.bam

^ The above command was used to map and sort (samtools) the reads relative to the reference genome, resulting in a BAM file. This was applied to the both the Plasmodium falciparum and the Escherichia Coli genome. The samtools flagstat function was applied to check for errors:

  > samtools flagstat output.bam

Next, bcftools was used to organise the output into .vcf format to list the detected mutations. 

  > bcftools mpileup -0u -f reference.fasta output.bam | bcftools call -ve -0v > output.vcf

The .vcf can be viewed as a text document within the terminal:

  > less -S output.vcf

Or can be indexed via samtools to enable viewing of the pileups reative to the sequence. 

  > samtools index output.vcf

  > samtools tview output.bam reference.fasta

### Comparing Simulated vs. Detected Mutations:

After the variant callers were applied, the resulting .vcf files were read into python to be compared with the original change log text files that were made when the nutations were generated. vcf_compare.py takes in both of these files and performs a comparison to return values for both precision and recall.  

The .vcf file is first read in as a list, where each entry represents a row of the file. Metadata is removed by filtering out lines with begin with '##' and each line is spliced by the '\t' tab delimiters present to extract only the first few columns (which contain the co-ordinate on the chromosome, and the detected mutations). The tab delimiters are removed from the strings. Next, the change log is read in, and special characters '{}' present from the original dictionary form are removed via the re package. At this stage, both the change log and the vcf file are list objects, where each entry contains the chromosome location, and the mutation.

Each line of both lists are then compared in a simple for-loop which iterates through each line (as both are ordered by coordinate) and  records the number of identical matches. Precision is calculated as the % of matching mutations found out of the original change log, and recall is calculated as the number of mutations detected by the variant caller divided by the true number of mutations simulated. 

## Part 2: Building a Pipeline

### Pipeline Script:

Python was used to build a bash-based pipeline in a script called pipeline.py. It is important to note that the pipeline assumes that python and conda are already correctly installed, and will only run on devices running Linux platform (it may work in others but the packages have problems with installation and often cause errors).

The pipeline works by creating two different conda environments, and installing that packages required. It then uses python f script (f''' 'bash command' ''') to turn a string into a working command, and to execute the pipeline. The reason for creating two conda environments (even though this means the pipeline can take a long time to run) is because the two variant callers, bfctools and snippy, appear to require different versions of python, as well as variations in packages. 

The script also contained checking functions to ensure that each stage runs successfully. It does so by checking the 'returncode' for each stage, as this should be empty as long as no errors have been returned. It also checks that the output file for bcftools is present using the os package to check that both the path is there and the file is not empty.

#### Bcftools:
The conda environment is created with packages minimap2, samtools, bcftools, and bedtools in their most recent versions (as of 10/12/2025). This is preformed via the sys packages' subprocess.run() function. After, python f string format (previously mentioned) is used to create a string containing the bash pipeline command, which is executed via subprocess.run().

#### Snippy:
The conda environment is created with python version 3.7.0 and packages samtools, bcftools, bedtools, snpEff version 4.3, snippy version 3.2 and minimap. Next, samtools is downgraded to version 1.3 in a separate bash command (NO idea why this happened, but this is the only way that it seemed to work for some reason). Finally, as before, an f string was used to chain together a bach command to run snippy, which creates a new directory called snippy_results containing the snps.vcf results file. 

### Combining VCFs:
The function vcf_combine defined in vcf_combine.py was used to generate a list of mutations detected from both bcftools and snippy. It stripped each line of both vcf files down to just the nucleotide point, and the nucleotide mutations detected, and then formatted them for direct comparison. After removing duplicates, a combined vcf file was created. 

